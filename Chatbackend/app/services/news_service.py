import json
import os
from datetime import datetime, timedelta
import traceback
from flask import current_app, jsonify
from app.utils.transform_utils import normalize_scores, process_emotion_stance_data
from .news_analysis_service import NewsAnalysisService
from .news_collection_service import NewsCollectionService
from app.extensions import db
import hashlib
import concurrent.futures
import threading
import time
from tqdm import tqdm
import pymongo

class NewsService:
    @staticmethod
    def load_news_data():
        """
        ä»MongoDBåŠ è½½æ–°é—»æ•°æ®
        
        Returns:
            dict: æ–°é—»æ•°æ®
        """
        try:
            # ä¼˜å…ˆä»hot_news_processedé›†åˆè·å–å¤„ç†åçš„çƒ­é—¨æ–°é—»
            latest = NewsCollectionService.get_latest_processed_news()
            if latest:
                return latest.get("data", [])
                
            # å¦‚æœæ²¡æœ‰å¤„ç†åçš„æ•°æ®ï¼Œåˆ™ä»è€é›†åˆè·å–
            news_data = list(db.transformed_news.find({}, {'_id': 0}))
            if not news_data:
                return {"error": "No news data found in database"}
            return news_data
        except Exception as e:
            return {"error": f"Error loading news data: {str(e)}"}
    
    @staticmethod
    def get_news_titles(max_news_per_platform=5):
        """
        ä»APIè·å–çƒ­é—¨æ–°é—»æ ‡é¢˜
        
        Args:
            max_news_per_platform (int): æ¯ä¸ªå¹³å°æœ€å¤šè·å–çš„æ–°é—»æ•°é‡ï¼Œé»˜è®¤ä¸º5
            
        Returns:
            dict: åŒ…å«å¹³å°åç§°å’Œå¯¹åº”çƒ­é—¨æ–°é—»æ ‡é¢˜çš„å­—å…¸
        """
        from ..utils.api_utils import fetch_news_titles
        
        title_url = current_app.config.get('NEWS_API_BASE_URL')
        return fetch_news_titles(title_url, max_news_per_platform)
    
    @staticmethod
    def analyze_news(max_workers=16, max_news_per_platform=5):
        """
        è·å–å¹¶åˆ†ææ–°é—»
        
        Args:
            max_workers (int): æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            max_news_per_platform (int): æ¯ä¸ªå¹³å°æœ€å¤šåˆ†æçš„æ–°é—»æ•°é‡
            
        Returns:
            dict: åˆ†æç»“æœ
        """
        # è·å–é…ç½®
        api_key = current_app.config.get('QWEN_API_KEY')
        base_url = current_app.config.get('QWEN_BASE_URL')
        model = current_app.config.get('QWEN_MODEL')
        title_url = current_app.config.get('NEWS_API_BASE_URL')
        
        # åˆ›å»ºåˆ†ææœåŠ¡
        analysis_service = NewsAnalysisService(api_key, base_url, model)
        
        # è·å–å¹¶åˆ†ææ–°é—»
        results = analysis_service.parallel_process(
            title_url=title_url,
            max_workers=max_workers,
            max_news_per_platform=max_news_per_platform
        )
        
        # ä¿å­˜ç»“æœåˆ°MongoDB
        NewsService.save_results_to_mongodb(results)
        
        return results
    
    @staticmethod
    def save_results_to_mongodb(results, collection_name="news_data"):
        """
        å°†åˆ†æç»“æœä¿å­˜åˆ°MongoDB
        
        Args:
            results (dict): åˆ†æç»“æœ
            collection_name (str): é›†åˆåç§°ï¼Œé»˜è®¤ä¸º'news_data'
            
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # è®°å½•ä¿å­˜æ—¶é—´
            timestamp = datetime.now()
            
            # å°†ç»“æœæŒ‰å¹³å°ä¿å­˜åˆ°MongoDB
            platforms_count = 0
            items_count = 0
            
            for platform, news_list in results.items():
                platforms_count += 1
                # ä¸ºæ¯æ¡æ–°é—»æ·»åŠ æ—¶é—´æˆ³å’Œå¹³å°ä¿¡æ¯
                for news_item in news_list:
                    news_item["saved_at"] = timestamp
                    news_item["platform"] = platform
                    items_count += 1
                    
                    # æ’å…¥æˆ–æ›´æ–°æ•°æ®åº“ä¸­çš„è®°å½•
                    # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦
                    db.news_data.update_one(
                        {"title": news_item["title"]},
                        {"$set": news_item},
                        upsert=True
                    )
            
            # åŒæ—¶å¤„ç†è½¬æ¢åçš„æ•°æ®
            transformed_data = NewsService._transform_data_for_display(results)
            for item in transformed_data:
                db.transformed_news.update_one(
                    {"id": item["id"]},
                    {"$set": item},
                    upsert=True
                )
            
            print(f"å·²ä¿å­˜ {platforms_count} ä¸ªå¹³å°çš„ {items_count} æ¡æ–°é—»åˆ°MongoDB")
            return True
            
        except Exception as e:
            print(f"ä¿å­˜åˆ°MongoDBå¤±è´¥: {str(e)}")
            return False
    
    @staticmethod
    def _transform_data_for_display(results):
        """
        è½¬æ¢æ•°æ®ä¸ºæ˜¾ç¤ºæ ¼å¼
        
        Args:
            results (dict): åŸå§‹åˆ†æç»“æœ
            
        Returns:
            list: è½¬æ¢åçš„æ•°æ®åˆ—è¡¨
        """
        transformed_data = []
        
        rank = 1
        for platform, news_list in results.items():
            for news_item in news_list:
                if "analysis" in news_item:
                    # å¤„ç†åˆ†ææ•°æ®
                    analysis = news_item["analysis"]
                    analysis["platform"] = platform
                    analysis["rank"] = rank
                    
                    # å¤„ç†æƒ…æ„Ÿå’Œç«‹åœºæ•°æ®
                    processed_analysis = process_emotion_stance_data(analysis)
                    transformed_data.append(processed_analysis)
                    
                    rank += 1
        
        # æŒ‰ç…§å‚ä¸åº¦æ’åº
        transformed_data.sort(key=lambda x: x.get("participants", 0), reverse=True)
        
        # é‡æ–°åˆ†é…æ’å
        for i, item in enumerate(transformed_data, 1):
            item["rank"] = i
            
        return transformed_data

    @staticmethod
    def process_news_data(data):
        """Process news data using the transformation utilities."""
        if isinstance(data, list):
            return [process_emotion_stance_data(item) for item in data]
        return data

    @staticmethod
    def get_top_news_last_7days(limit=50):
        """
        è·å–è¿‘7å¤©å†…çƒ­åº¦æœ€é«˜çš„æ–°é—»
        
        Args:
            limit (int): é™åˆ¶è¿”å›çš„è®°å½•æ•°é‡ï¼Œé»˜è®¤50æ¡
            
        Returns:
            list: çƒ­åº¦æœ€é«˜çš„æ–°é—»åˆ†ææ•°æ®åˆ—è¡¨
        """
        try:
            from datetime import datetime, timedelta
            from ..models import db
            
            # è®¡ç®—7å¤©å‰çš„æ—¶é—´
            seven_days_ago = datetime.now() - timedelta(days=7)
            
            # ä¸»è¦ä»transformed_newsé›†åˆè·å–å·²åˆ†ææ•°æ®ï¼ˆå·²ç»åŒ…å«å®Œæ•´åˆ†æç»“æœï¼‰
            analyzed_items = list(db.transformed_news.find(
                {}, 
                {'_id': 0}
            ).sort("participants", -1).limit(limit))
            
            # å¦‚æœå·²åˆ†ææ•°æ®ä¸è¶³ï¼Œåˆ™æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šçš„åŸå§‹æ–°é—»æ•°æ®
            if len(analyzed_items) < limit:
                # æŸ¥è¯¢æ¡ä»¶ï¼š7å¤©å†…çš„æ–°é—»
                query = {"saved_at": {"$gte": seven_days_ago}}
                
                # ä»news_dataé›†åˆä¸­è·å–æ•°æ®
                news_items = list(db.news_data.find(
                    query, 
                    {'_id': 0}
                ).sort("composite_hot" if "composite_hot" in db.news_data.find_one({}) else "weighted_hot_sum", -1).limit(limit))
                
                # æ”¶é›†æ‰€æœ‰å·²æœ‰åˆ†æçš„æ–°é—»æ ‡é¢˜
                analyzed_titles = {item.get("title") for item in analyzed_items}
                
                # æŸ¥æ‰¾æ–°é—»åˆ†æç»“æœ
                for item in news_items:
                    title = item.get("title")
                    if title and title not in analyzed_titles:
                        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„åˆ†ææ•°æ®åœ¨news_daily_analysisé›†åˆä¸­
                        analysis = db.news_daily_analysis.find_one(
                            {"title": title}, 
                            {'_id': 0, 'analysis': 1}
                        )
                        
                        if analysis and 'analysis' in analysis:
                            analyzed_items.append(analysis['analysis'])
                            analyzed_titles.add(title)
                            
                            # å¦‚æœè¾¾åˆ°é™åˆ¶ï¼Œåœæ­¢æ·»åŠ 
                            if len(analyzed_items) >= limit:
                                break
            
            # ç¡®ä¿æ‰€æœ‰é¡¹ç›®éƒ½åŒ…å«å¿…è¦çš„å­—æ®µ
            for item in analyzed_items:
                if 'id' not in item or not item['id']:
                    item['id'] = hashlib.md5(item.get('title', '').encode()).hexdigest()
            
            # æŒ‰å‚ä¸åº¦æ’åºï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            analyzed_items.sort(key=lambda x: x.get('participants', 0), reverse=True)
            
            return analyzed_items
            
        except Exception as e:
            print(f"è·å–è¿‘7å¤©çƒ­é—¨æ–°é—»å¤±è´¥: {str(e)}")
            return []

    @staticmethod
    def get_news_titles_for_analysis(max_news_per_platform=None):
        """
        è·å–å¾…åˆ†æçš„æ–°é—»æ ‡é¢˜
        
        ä¼˜å…ˆä»MongoDBçš„hot_news_processedé›†åˆè·å–æ•°æ®ï¼ŒæŒ‰ç»¼åˆçƒ­åº¦æ’åºå–å‰næ¡
        
        Args:
            max_news_per_platform: è¦æ£€ç´¢çš„æœ€å¤§æ–°é—»æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„TOP_HOT_NEWS_COUNTå€¼
                                  å‚æ•°åä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç ï¼Œå®é™…ä½œç”¨æ˜¯è·å–top_nçƒ­åº¦æ–°é—»

        Returns:
            list: åŒ…å«æ ¼å¼åŒ–å¥½çš„æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸ï¼ŒåŒ…å«title, platform, url
        """
        try:
            # ä»ç¯å¢ƒå˜é‡è·å–çƒ­åº¦å‰næ¡çš„é…ç½®
            if max_news_per_platform is None:
                from flask import current_app
                # ä¼˜å…ˆä½¿ç”¨TOP_HOT_NEWS_COUNTé…ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å›é€€åˆ°MAX_NEWS_PER_PLATFORM
                top_n = current_app.config.get('TOP_HOT_NEWS_COUNT')
                if top_n is None:
                    top_n = current_app.config.get('MAX_NEWS_PER_PLATFORM', 10)
                print(f"è·å–çƒ­åº¦æ’åå‰ {top_n} æ¡æ–°é—»")
            else:
                top_n = max_news_per_platform
                
            print(f"ä»æ•°æ®åº“è·å–çƒ­åº¦æ’åå‰{top_n}æ¡æ–°é—»")
            
            # ä»MongoDBè·å–æœ€æ–°å¤„ç†è¿‡çš„çƒ­é—¨æ–°é—»æ•°æ®
            from ..services.news_collection_service import NewsCollectionService
            latest_processed = NewsCollectionService.get_latest_processed_news()
            
            if latest_processed and "data" in latest_processed:
                # è·å–ç»¼åˆçƒ­åŠ›æ¦œ
                for platform_data in latest_processed["data"]:
                    if platform_data.get("platform") == "comprehensive":
                        news_data = platform_data.get("data", [])
                        
                        if news_data and len(news_data) > 0:
                            # æŒ‰ç»¼åˆçƒ­åº¦é™åºæ’åºï¼ˆç¡®ä¿æ’åºæ­£ç¡®ï¼‰
                            sorted_news = sorted(news_data, key=lambda x: x.get("comprehensive_heat", 0), reverse=True)
                            
                            # è·å–å‰næ¡æ–°é—»
                            top_news = sorted_news[:top_n]
                            
                            # æ ¼å¼åŒ–ä¸ºåˆ†ææ‰€éœ€çš„æ ¼å¼
                            formatted_news = [{"title": news.get("title", ""), 
                                             "platform": news.get("platforms", ["unknown"])[0] if news.get("platforms") else "unknown", 
                                             "url": news.get("url", "")} 
                                            for news in top_news]
                            
                            print(f"ä»æ•°æ®åº“è·å–äº†{len(formatted_news)}æ¡å¾…åˆ†æçš„çƒ­é—¨æ–°é—»")
                            return formatted_news
            
            # å¦‚æœæ²¡æœ‰å¤„ç†è¿‡çš„æ•°æ®ï¼Œè¿”å›ç©ºåˆ—è¡¨
            print("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°çƒ­é—¨æ–°é—»æ•°æ®")
            return []
            
        except Exception as e:
            print(f"è·å–æ–°é—»æ ‡é¢˜å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
            
    @staticmethod
    def _get_titles_from_api(max_news_per_platform=5):
        """
        ä»APIè·å–çƒ­é—¨æ–°é—»æ ‡é¢˜
        
        Args:
            max_news_per_platform (int): æ¯ä¸ªå¹³å°æœ€å¤šè·å–çš„æ–°é—»æ•°é‡
            
        Returns:
            list: æ–°é—»æ ‡é¢˜åˆ—è¡¨
        """
        try:
            from ..utils.api_utils import fetch_news_titles
            from flask import current_app
            
            title_url = current_app.config.get('NEWS_API_BASE_URL')
            result = fetch_news_titles(title_url, max_news_per_platform)
            
            # è½¬æ¢ä¸ºåˆ†ææ‰€éœ€çš„æ ¼å¼
            formatted_news = []
            for platform, news_list in result.items():
                for news in news_list:
                    formatted_news.append({
                        "title": news.get("title", ""),
                        "platform": platform,
                        "url": news.get("url", "")
                    })
            
            print(f"ä»APIè·å–äº†{len(formatted_news)}æ¡æ–°é—»æ ‡é¢˜")
            return formatted_news
        except Exception as e:
            print(f"ä»APIè·å–æ–°é—»æ ‡é¢˜å¤±è´¥: {str(e)}")
            return []
    
    @staticmethod
    def check_news_in_database(news_items, max_age=24):
        """
        æ£€æŸ¥æ–°é—»æ ‡é¢˜åœ¨æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨æœ€æ–°åˆ†æï¼Œä¼˜åŒ–ä¸ºå•æ¬¡æŸ¥è¯¢ä»¥å‡å°‘æ•°æ®åº“è´Ÿè½½
        
        Args:
            news_items (list): æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«titleå’Œplatform
            max_age (int): æœ€å¤§æœ‰æ•ˆæ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é—´è§†ä¸ºéœ€è¦æ›´æ–°
            
        Returns:
            tuple: (å·²æœ‰åˆ†æçš„æ–°é—», éœ€è¦åˆ†æçš„æ–°é—»)
        """
        from ..models import db
        
        # è®¡ç®—æ—¶é—´é˜ˆå€¼
        max_age_time = datetime.now() - timedelta(hours=max_age)
        
        # æå–æ‰€æœ‰æ ‡é¢˜å¹¶å‡†å¤‡æŸ¥è¯¢
        all_titles = [item.get("title") for item in news_items if item.get("title")]
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ‡é¢˜ï¼Œç›´æ¥è¿”å›
        if not all_titles:
            return [], []
        
        # å•æ¬¡æŸ¥è¯¢è·å–æ‰€æœ‰å·²å­˜åœ¨çš„åˆ†æ
        existing_analyses = list(db.transformed_news.find({"title": {"$in": all_titles}}))
        
        # åˆ›å»ºæ ‡é¢˜åˆ°åˆ†æçš„æ˜ å°„
        title_to_analysis = {}
        for analysis in existing_analyses:
            title_to_analysis[analysis.get("title")] = analysis
        
        # åˆ†ç±»å¤„ç†æ¯ä¸ªæ–°é—»é¡¹
        existing_news = []
        news_to_analyze = []
        
        for item in news_items:
            title = item.get("title")
            if not title:
                continue
                
            analysis = title_to_analysis.get(title)
            
            if analysis:
                # æ£€æŸ¥åˆ†ææ—¶é—´æ˜¯å¦æœ€æ–°
                analyzed_at = analysis.get("analyzed_at")
                if isinstance(analyzed_at, str):
                    try:
                        analyzed_at = datetime.fromisoformat(analyzed_at)
                    except ValueError:
                        analyzed_at = datetime.min
                else:
                    analyzed_at = datetime.min
                
                if analyzed_at > max_age_time:
                    # åˆ†æç»“æœè¶³å¤Ÿæ–°
                    existing_news.append(analysis)
                else:
                    # éœ€è¦æ›´æ–°åˆ†æ
                    news_to_analyze.append(item)
            else:
                # æ•°æ®åº“ä¸­æ²¡æœ‰æ­¤æ–°é—»
                news_to_analyze.append(item)
        
        return existing_news, news_to_analyze
    
    @staticmethod
    def analyze_specific_news(news_items, max_workers=16, timeout=60):
        """
        åˆ†ææŒ‡å®šçš„æ–°é—»åˆ—è¡¨ï¼Œä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿå¤„ç†ï¼Œä¼˜åŒ–é˜²æ­¢é‡å¤å¤„ç†
        
        Args:
            news_items (list): å¾…åˆ†æçš„æ–°é—»åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«titleå’Œplatform
            max_workers (int): æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            timeout (int): APIè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            list: åˆ†æç»“æœ
        """
        if not news_items:
            return []
        
        # è·å–å½“å‰åº”ç”¨å®ä¾‹ï¼Œç”¨äºçº¿ç¨‹å†…åˆ›å»ºä¸Šä¸‹æ–‡
        app = current_app._get_current_object()
        
        # è·å–é…ç½®
        api_key = current_app.config.get('QWEN_API_KEY')
        base_url = current_app.config.get('QWEN_BASE_URL')
        model = current_app.config.get('QWEN_MODEL')
        
        # åˆ›å»ºåˆ†ææœåŠ¡
        from .news_analysis_service import NewsAnalysisService
        analysis_service = NewsAnalysisService(api_key, base_url, model)
        
        # çº¿ç¨‹å®‰å…¨çš„ç»“æœåˆ—è¡¨å’Œå¤„ç†è®°å½•
        analyzed_results = []
        result_lock = threading.Lock()
        
        # ä½¿ç”¨é›†åˆè®°å½•å·²å¤„ç†æˆ–æ­£åœ¨å¤„ç†çš„æ–°é—»æ ‡é¢˜ï¼Œé˜²æ­¢å¤šçº¿ç¨‹é‡å¤å¤„ç†
        processing_news = set()
        processing_lock = threading.Lock()
        
        # APIè°ƒç”¨ç›‘æ§
        api_stats = {
            "total": 0,
            "success": 0,
            "timeout": 0,
            "error": 0,
            "avg_duration": 0,
            "durations": []
        }
        stats_lock = threading.Lock()
        
        # è¿‡æ»¤æ‰é‡å¤çš„æ–°é—»æ ‡é¢˜
        filtered_news_items = []
        for item in news_items:
            title = item.get("title", "").strip()
            if not title:
                continue
                
            with processing_lock:
                if title not in processing_news:
                    processing_news.add(title)
                    filtered_news_items.append(item)
        
        # æ— æ–°é—»éœ€è¦å¤„ç†
        if not filtered_news_items:
            return []
            
        # è®¡ç®—æœ‰æ•ˆçš„çº¿ç¨‹æ•°ï¼Œé¿å…å¯åŠ¨è¿‡å¤šçº¿ç¨‹
        effective_workers = min(max_workers, len(filtered_news_items))
        print(f"ä½¿ç”¨{effective_workers}ä¸ªçº¿ç¨‹å¹¶è¡Œåˆ†æ{len(filtered_news_items)}æ¡æ–°é—»")
        
        # åˆ†æå•ä¸ªæ–°é—»çš„å‡½æ•°
        def analyze_one_news(item):
            # è·å–æ–°é—»æ ‡é¢˜å’ŒIDï¼Œç”¨äºå”¯ä¸€è¯†åˆ«
            title = item["title"]
            news_id = hashlib.md5(title.encode()).hexdigest()
            
            # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡
            with app.app_context():
                start_time = time.time()
                
                try:
                    print(f"å¼€å§‹åˆ†ææ–°é—»: {title}")
                    
                    # åˆ›å»ºåŸºæœ¬æ–°é—»æ•°æ®
                    news_data = {
                        "id": news_id,
                        "title": title
                    }
                    
                    # æ ‡è®°APIè°ƒç”¨å¼€å§‹
                    with stats_lock:
                        api_stats["total"] += 1
                    
                    # ä½¿ç”¨æµå¼åˆ†ææ–¹æ³•ï¼Œæä¾›è¶…æ—¶ä¿æŠ¤
                    try:
                        # è®¾ç½®è¶…æ—¶å®šæ—¶å™¨
                        result = analysis_service.analyze_news(news_data)
                        
                        # æ›´æ–°APIç»Ÿè®¡
                        api_duration = time.time() - start_time
                        with stats_lock:
                            api_stats["success"] += 1
                            api_stats["durations"].append(api_duration)
                            total_durations = sum(api_stats["durations"])
                            count_durations = len(api_stats["durations"])
                            api_stats["avg_duration"] = total_durations / count_durations
                    
                    except TimeoutError:
                        with stats_lock:
                            api_stats["timeout"] += 1
                        print(f"â±ï¸ åˆ†ææ–°é—»'{title}'è¶…æ—¶ (>{timeout}ç§’)")
                        # # ä½¿ç”¨åå¤‡æ–¹æ¡ˆç”Ÿæˆæ•°æ®
                        # from ..utils.data_utils import generate_fallback_data
                        # result = generate_fallback_data(title)
                    
                    # æ·»åŠ å¹³å°ä¿¡æ¯å’Œåˆ†ææ—¶é—´
                    result["platform"] = item.get("platform", "unknown")
                    result["analyzed_at"] = datetime.now().isoformat()
                    result["title"] = title  # ç¡®ä¿ç»“æœä¸­åŒ…å«title
                    
                    # ä¿å­˜åˆ°æ•°æ®åº“ (å·²åœ¨app_contextå†…)
                    db.transformed_news.update_one(
                        {"title": title},
                        {"$set": result},
                        upsert=True
                    )
                    
                    # å®‰å…¨åœ°æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                    with result_lock:
                        analyzed_results.append(result)
                    
                    print(f"æ–°é—»'{title}'åˆ†æå®Œæˆ")
                    return result
                    
                except Exception as e:
                    with stats_lock:
                        api_stats["error"] += 1
                    print(f"åˆ†ææ–°é—»'{title}'å¤±è´¥: {str(e)}")
                    
                    try:
                        # å°è¯•ç”Ÿæˆåå¤‡æ•°æ®
                        from ..utils.data_utils import generate_fallback_data
                        fallback = generate_fallback_data(title)
                        fallback["platform"] = item.get("platform", "unknown")
                        fallback["analyzed_at"] = datetime.now().isoformat()
                        fallback["title"] = title  # ç¡®ä¿ç»“æœä¸­åŒ…å«title
                        
                        # ä¿å­˜åå¤‡æ•°æ®åˆ°æ•°æ®åº“
                        db.transformed_news.update_one(
                            {"title": title},
                            {"$set": fallback},
                            upsert=True
                        )
                        
                        with result_lock:
                            analyzed_results.append(fallback)
                        
                        return fallback
                    except Exception as e2:
                        print(f"ç”Ÿæˆåå¤‡æ•°æ®ä¹Ÿå¤±è´¥: {str(e2)}")
                        return None
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†åˆ†æä»»åŠ¡ï¼Œå¸¦è¿›åº¦æ¡
        with concurrent.futures.ThreadPoolExecutor(max_workers=effective_workers) as executor, \
             tqdm(total=len(filtered_news_items), desc="åˆ†ææ–°é—»", unit="æ¡") as pbar:
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = {}
            for item in filtered_news_items:
                futures[executor.submit(analyze_one_news, item)] = item
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in concurrent.futures.as_completed(futures):
                item = futures[future]
                try:
                    # è¿™é‡Œæˆ‘ä»¬å¹¶ä¸ä½¿ç”¨future.result()çš„è¿”å›å€¼
                    # å› ä¸ºç»“æœå·²ç»åœ¨analyze_one_newså‡½æ•°ä¸­æ·»åŠ åˆ°analyzed_resultsä¸­
                    future.result(timeout=timeout + 10)  # ç»™äºˆé¢å¤–çš„10ç§’å®Œæˆæ—¶é—´
                except concurrent.futures.TimeoutError:
                    print(f"çº¿ç¨‹å¤„ç†æ–°é—»'{item['title']}'è¶…æ—¶")
                except Exception as e:
                    print(f"å¤„ç†æ–°é—»'{item['title']}'æ—¶å‡ºé”™: {str(e)}")
                finally:
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.update(1)
                    
                    # å®šæœŸè¾“å‡ºAPIè°ƒç”¨ç»Ÿè®¡
                    if pbar.n % 5 == 0 or pbar.n == len(filtered_news_items):
                        success_rate = 0
                        if api_stats["total"] > 0:
                            success_rate = (api_stats["success"] / api_stats["total"]) * 100
                        avg_time = api_stats["avg_duration"] if api_stats["durations"] else 0
                        print(f"\nğŸ“Š APIç›‘æ§ - å®Œæˆ: {pbar.n}/{len(filtered_news_items)} | æˆåŠŸç‡: {success_rate:.1f}% | å¹³å‡æ—¶é—´: {avg_time:.2f}ç§’ | è¶…æ—¶: {api_stats['timeout']} | é”™è¯¯: {api_stats['error']}")
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        print(f"å¤šçº¿ç¨‹åˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†æ{len(analyzed_results)}/{len(filtered_news_items)}æ¡æ–°é—»")
        if api_stats["total"] > 0:
            success_rate = api_stats["success"] / api_stats["total"] * 100
            print(f"APIè°ƒç”¨æˆåŠŸç‡: {success_rate:.1f}%ï¼Œå¹³å‡å“åº”æ—¶é—´: {api_stats['avg_duration']:.2f}ç§’")
        
        # è¿”å›æ’åºåçš„ç»“æœ
        return sorted(analyzed_results, key=lambda x: x.get("participants", 0), reverse=True)
    
    @staticmethod
    def get_and_analyze_news(max_news_per_platform=2, max_workers=16, max_age=24):
        """
        è·å–å¹¶åˆ†ææ–°é—»æ•°æ®çš„å®Œæ•´æµç¨‹ï¼ŒæŒ‰ç…§è§£è€¦æ¶æ„è®¾è®¡:
        1. è·å–æ–°é—»æ ‡é¢˜ï¼ˆIOæ“ä½œï¼‰
        2. æ£€æŸ¥æ•°æ®åº“ç¼“å­˜ï¼ˆDBæ“ä½œï¼‰
        3. åˆ†æéœ€è¦åˆ†æçš„æ–°é—»ï¼ˆè®¡ç®—æ“ä½œï¼‰
        4. åˆå¹¶ç»“æœå¹¶æ’åºï¼ˆå†…å­˜æ“ä½œï¼‰
        
        Args:
            max_news_per_platform (int): æ¯ä¸ªå¹³å°æœ€å¤šåˆ†æçš„æ–°é—»æ•°é‡
            max_workers (int): æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            max_age (int): æœ€å¤§æœ‰æ•ˆæ—¶é—´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            list: åˆ†æç»“æœ
        """
        try:
            # 1. è·å–æ–°é—»æ ‡é¢˜
            news_items = NewsService.get_news_titles_for_analysis(max_news_per_platform)
            if not news_items:
                print("æœªèƒ½è·å–åˆ°æ–°é—»æ ‡é¢˜ï¼Œè¿”å›ç©ºç»“æœ")
                return []
            
            print(f"è·å–åˆ° {len(news_items)} æ¡æ–°é—»æ ‡é¢˜")
            
            # 2. æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰åˆ†æ - è½¬æ¢ä¸ºé›†åˆæ“ä½œé¿å…SQL N+1é—®é¢˜
            existing_news, news_to_analyze = NewsService.check_news_in_database(news_items, max_age)
            print(f"æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_news)} æ¡æ–°é—»çš„æœ€æ–°åˆ†æï¼Œéœ€è¦åˆ†æ {len(news_to_analyze)} æ¡æ–°é—»")
            
            # å¦‚æœæ²¡æœ‰éœ€è¦åˆ†æçš„æ–°é—»ï¼Œç›´æ¥è¿”å›å·²æœ‰ç»“æœ
            if not news_to_analyze:
                print("æ‰€æœ‰æ–°é—»éƒ½å·²æœ‰æœ€æ–°åˆ†æï¼Œæ— éœ€å†æ¬¡åˆ†æ")
                return sorted(existing_news, key=lambda x: x.get("participants", 0), reverse=True)
            
            # 3. åˆ†æéœ€è¦åˆ†æçš„æ–°é—» - ä½¿ç”¨ä¼˜åŒ–åçš„å¤šçº¿ç¨‹æ–¹æ³•
            new_analyzed = []
            if news_to_analyze:
                print(f"å¼€å§‹åˆ†æ {len(news_to_analyze)} æ¡æ–°é—»...")
                
                # åˆ›å»ºåˆ†ææœåŠ¡
                api_key = current_app.config.get('QWEN_API_KEY')
                base_url = current_app.config.get('QWEN_BASE_URL')
                model = current_app.config.get('QWEN_MODEL')
                
                analysis_service = NewsAnalysisService(api_key, base_url, model)
                
                # æå–titleså’Œplatformsåˆ—è¡¨ç”¨äºæ‰¹é‡åˆ†æ
                titles = [item.get("title") for item in news_to_analyze]
                platforms = [item.get("platform") for item in news_to_analyze]
                
                # ä½¿ç”¨ä¼˜åŒ–çš„å¤šçº¿ç¨‹åˆ†ææ–¹æ³•
                new_analyzed = analysis_service.analyze_multiple_news(
                    titles, platforms, max_workers=max_workers
                )
                
                print(f"æˆåŠŸåˆ†æ {len(new_analyzed)} æ¡æ–°é—»")
            
            # 4. åˆå¹¶ç»“æœå¹¶æ’åº
            all_results = existing_news + new_analyzed
            
            # æŒ‰å‚ä¸çƒ­åº¦æ’åº
            sorted_results = sorted(all_results, key=lambda x: x.get("participants", 0), reverse=True)
            
            return sorted_results
            
        except Exception as e:
            print(f"è·å–å’Œåˆ†ææ–°é—»æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            
            # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›å·²æœ‰çš„ç»“æœ
            if existing_news:
                return sorted(existing_news, key=lambda x: x.get("participants", 0), reverse=True)
            return []

    @staticmethod
    def check_valid_news_count(min_count=50, days=7):
        """
        æ£€æŸ¥æ•°æ®åº“ä¸­æœ€è¿‘å‡ å¤©çš„æœ‰æ•ˆæ–°é—»æ•°é‡ï¼Œå¦‚æœå°‘äºæŒ‡å®šæ•°é‡ï¼Œè§¦å‘æ–°é—»åˆ†æä»»åŠ¡
        
        Args:
            min_count (int): æœ€å°æœ‰æ•ˆæ–°é—»æ•°é‡ï¼Œé»˜è®¤50æ¡
            days (int): æ£€æŸ¥çš„å¤©æ•°èŒƒå›´ï¼Œé»˜è®¤7å¤©
            
        Returns:
            dict: æ£€æŸ¥ç»“æœ
        """
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            cutoff_time = datetime.now() - timedelta(days=days)
            
            # æŸ¥è¯¢æ•°æ®åº“ä¸­æœ€è¿‘dayså¤©çš„æ–°é—»
            # æ³¨æ„: æŸ¥è¯¢éfallbackæ•°æ®ï¼Œå³is_fallbackå­—æ®µä¸å­˜åœ¨æˆ–ä¸ºFalse
            valid_news_count = db.transformed_news.count_documents({
                "$and": [
                    {"analyzed_at": {"$gte": cutoff_time.isoformat()}},
                    {"$or": [
                        {"is_fallback": {"$exists": False}},
                        {"is_fallback": False}
                    ]}
                ]
            })
            
            print(f"æ•°æ®åº“ä¸­è¿‘{days}å¤©çš„æœ‰æ•ˆæ–°é—»æ•°é‡: {valid_news_count}")
            
            # å¦‚æœæœ‰æ•ˆæ–°é—»æ•°é‡å°‘äºæœ€å°è¦æ±‚
            if valid_news_count < min_count:
                print(f"æœ‰æ•ˆæ–°é—»æ•°é‡ä¸è¶³ ({valid_news_count}/{min_count})ï¼Œå¯åŠ¨åˆ†æä»»åŠ¡")
                
                # è®¡ç®—éœ€è¦åˆ†æçš„æ–°é—»æ•°é‡
                news_to_analyze_count = min_count - valid_news_count
                
                # ä¼°ç®—æ¯ä¸ªå¹³å°éœ€è¦è·å–çš„æ–°é—»æ•°é‡
                # å‡è®¾æœ‰5ä¸ªå¹³å°ï¼Œåˆ™æ¯ä¸ªå¹³å°éœ€è¦ news_to_analyze_count / 5 æ¡
                max_news_per_platform = max(1, news_to_analyze_count // 5)
                
                # è°ƒç”¨å®‰æ’åˆ†æä»»åŠ¡çš„æ–¹æ³•
                schedule_result = NewsService.schedule_news_analysis(
                    max_news_per_platform=max_news_per_platform
                )
                
                # ç«‹å³å¤„ç†ä¸€éƒ¨åˆ†é˜Ÿåˆ—
                process_result = NewsService.process_analysis_queue(
                    max_workers=16, 
                    limit=min(10, news_to_analyze_count)
                )
                
                return {
                    "status": "scheduled",
                    "valid_count": valid_news_count,
                    "required_count": min_count,
                    "scheduled_result": schedule_result,
                    "process_result": process_result
                }
            else:
                return {
                    "status": "sufficient",
                    "valid_count": valid_news_count,
                    "required_count": min_count
                }
                
        except Exception as e:
            print(f"æ£€æŸ¥æœ‰æ•ˆæ–°é—»æ•°é‡å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}
    
    @staticmethod
    def get_hot_news():
        """
        è·å–çƒ­é—¨æ–°é—»ï¼Œä»…ä»æ•°æ®åº“è·å–å·²å¤„ç†å¥½çš„æ•°æ®
        
        Returns:
            dict: åŒ…å«çƒ­é—¨æ–°é—»æ•°æ®çš„å­—å…¸
        """
        try:
            # ä»æ•°æ®åº“è·å–æœ€è¿‘å¤„ç†çš„çƒ­é—¨æ–°é—»
            from ..services.news_collection_service import NewsCollectionService
            latest_news = NewsCollectionService.get_latest_processed_news()
            
            if latest_news:
                # æŒ‰çƒ­åº¦æ’åº
                for platform_data in latest_news.get("data", []):
                    if platform_data.get("platform") == "comprehensive":
                        news_list = platform_data.get("data", [])
                        # ç¡®ä¿æŒ‰çƒ­åº¦æ’åº
                        sorted_news = sorted(news_list, key=lambda x: x.get("comprehensive_heat", 0), reverse=True)
                        platform_data["data"] = sorted_news
                
                return {
                    "success": True,
                    "source": "database",
                    "count": latest_news.get("total_news", 0),
                    "data": latest_news.get("data", [])
                }
            
            # æ²¡æœ‰æ‰¾åˆ°æ•°æ®
            return {
                "success": False,
                "error": "No hot news data available in database"
            }
            
        except Exception as e:
            print(f"è·å–çƒ­é—¨æ–°é—»å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": str(e)
            }

    @staticmethod
    def get_all_news():
        """è·å–æ‰€æœ‰æ–°é—»"""
        try:
            news_collection = db.news_collection
            news_items = list(news_collection.find({}, {'_id': 0}))
            return jsonify({"success": True, "data": news_items})
        except Exception as e:
            print(f"Error: {str(e)}")
            return jsonify({"success": False, "error": str(e)}), 500

    @staticmethod
    def get_news_by_id(news_id):
        """æ ¹æ®IDè·å–æ–°é—»"""
        try:
            news_collection = db.news_collection
            news_item = news_collection.find_one({"id": news_id}, {'_id': 0})
            if news_item:
                return jsonify({"success": True, "data": news_item})
            else:
                return jsonify({"success": False, "error": "News not found"}), 404
        except Exception as e:
            print(f"Error: {str(e)}")
            return jsonify({"success": False, "error": str(e)}), 500

    @staticmethod
    def search_news(keyword):
        """æœç´¢æ–°é—»"""
        try:
            if not keyword:
                return jsonify({"success": False, "error": "Keyword is required"}), 400

            news_collection = db.news_collection
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œæ¨¡ç³Šæœç´¢
            news_items = list(news_collection.find(
                {"title": {"$regex": keyword, "$options": "i"}},
                {'_id': 0}
            ))
            return jsonify({"success": True, "data": news_items})
        except Exception as e:
            print(f"Error: {str(e)}")
            return jsonify({"success": False, "error": str(e)}), 500

    @staticmethod
    def schedule_news_analysis(max_news_per_platform=None, max_workers=16):
        """
        å®‰æ’æ–°é—»åˆ†æä»»åŠ¡ï¼Œå¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ç”¨æˆ·è¯·æ±‚
        æ­¤æ–¹æ³•åº”è¯¥é€šè¿‡åå°ä»»åŠ¡è°ƒåº¦å™¨å®šæœŸè°ƒç”¨ï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨APIè¯·æ±‚ä¸­è°ƒç”¨
        
        åˆ†æä»…ä½¿ç”¨æ•°æ®åº“hot_news_processedé›†åˆä¸­çš„çƒ­é—¨æ–°é—»ï¼Œä¸ä¼šä»APIè·å–æ–°æ•°æ®
        
        Args:
            max_news_per_platform: è¦è·å–çš„çƒ­åº¦æœ€é«˜æ–°é—»æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨TOP_HOT_NEWS_COUNTé…ç½®
                                   å‚æ•°åä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç ï¼Œå®é™…ä½œç”¨æ˜¯è·å–top_nçƒ­åº¦æ–°é—»
            max_workers (int): æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            dict: ä»»åŠ¡å®‰æ’çŠ¶æ€
        """
        try:
            # 1. è·å–æ–°é—»æ ‡é¢˜ - ä»æ•°æ®åº“hot_news_processedé›†åˆ
            news_items = NewsService.get_news_titles_for_analysis(max_news_per_platform)
            if not news_items:
                print("æœªèƒ½ä»æ•°æ®åº“è·å–åˆ°çƒ­é—¨æ–°é—»æ ‡é¢˜ï¼Œæ— æ³•å®‰æ’åˆ†æä»»åŠ¡")
                return {"status": "error", "message": "æœªèƒ½ä»æ•°æ®åº“è·å–åˆ°çƒ­é—¨æ–°é—»æ ‡é¢˜"}
            
            print(f"ä»æ•°æ®åº“è·å–åˆ° {len(news_items)} æ¡çƒ­é—¨æ–°é—»æ ‡é¢˜ï¼Œå‡†å¤‡å®‰æ’åˆ†æ")
            
            # 2. æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰åˆ†æ - é¿å…é‡å¤åˆ†æ
            existing_titles = set()
            recent_cutoff = datetime.now() - timedelta(hours=24)
            
            # æŸ¥è¯¢æœ€è¿‘24å°æ—¶å†…å·²åˆ†æçš„æ–°é—»
            existing_analyses = list(db.transformed_news.find(
                {"analyzed_at": {"$gte": recent_cutoff.isoformat()}},
                {"title": 1, "_id": 0}
            ))
            
            for analysis in existing_analyses:
                if "title" in analysis:
                    existing_titles.add(analysis["title"])
            
            # è¿‡æ»¤å‡ºéœ€è¦åˆ†æçš„æ–°é—»
            news_to_analyze = []
            for item in news_items:
                title = item.get("title", "").strip()
                if title and title not in existing_titles:
                    news_to_analyze.append(item)
                    existing_titles.add(title)  # é¿å…åˆ—è¡¨ä¸­çš„é‡å¤
            
            # å¦‚æœå…¨éƒ¨å·²åˆ†æï¼Œç›´æ¥è¿”å›
            if not news_to_analyze:
                print("æ‰€æœ‰è·å–çš„æ–°é—»éƒ½å·²æœ‰åˆ†æï¼Œæ— éœ€å†æ¬¡åˆ†æ")
                return {"status": "skipped", "message": "æ‰€æœ‰æ–°é—»å·²æœ‰åˆ†æ"}
            
            # 3. å°†å¾…åˆ†ææ–°é—»ä¿å­˜åˆ°åˆ†æé˜Ÿåˆ—é›†åˆä¸­
            timestamp = datetime.now()
            for item in news_to_analyze:
                # ç”Ÿæˆå”¯ä¸€ID
                news_id = hashlib.md5(item.get("title", "").encode()).hexdigest()
                
                # ä¿å­˜åˆ°åˆ†æé˜Ÿåˆ—
                db.news_analysis_queue.update_one(
                    {"news_id": news_id},
                    {"$set": {
                        "news_id": news_id,
                        "news_data": item,
                        "status": "pending",
                        "queued_at": timestamp.isoformat(),
                        "attempts": 0,
                        "last_attempt": None
                    }},
                    upsert=True
                )
            
            print(f"æˆåŠŸå°† {len(news_to_analyze)} æ¡æ–°é—»åŠ å…¥åˆ†æé˜Ÿåˆ—")
            
            # 4. ç«‹å³å¤„ç†é˜Ÿåˆ—ä¸­çš„æ–°é—»
            print("ç«‹å³å¼€å§‹å¤„ç†é˜Ÿåˆ—ä¸­çš„æ–°é—»...")
            process_result = NewsService.process_queue_immediately(max_workers)
            
            return {
                "status": "processing", 
                "message": f"å·²å°†{len(news_to_analyze)}æ¡æ–°é—»åŠ å…¥åˆ†æé˜Ÿåˆ—å¹¶ç«‹å³å¼€å§‹å¤„ç†",
                "queued": len(news_to_analyze),
                "process_result": process_result
            }
            
        except Exception as e:
            print(f"å®‰æ’æ–°é—»åˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}
    
    @staticmethod
    def process_queue_immediately(max_workers=16):
        """
        ç«‹å³å¤„ç†é˜Ÿåˆ—ä¸­çš„æ–°é—»ï¼Œä¸ç­‰å¾…å®šæ—¶ä»»åŠ¡
        
        Args:
            max_workers (int): æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        try:
            # æ£€æŸ¥é˜Ÿåˆ—ä¸­å¾…å¤„ç†çš„æ–°é—»æ•°é‡
            pending_count = db.news_analysis_queue.count_documents({"status": "pending"})
            
            if pending_count == 0:
                return {"status": "empty", "message": "åˆ†æé˜Ÿåˆ—ä¸ºç©ºï¼Œæ— éœ€å¤„ç†"}
                
            print(f"é˜Ÿåˆ—ä¸­æœ‰ {pending_count} æ¡å¾…å¤„ç†æ–°é—»ï¼Œç«‹å³å¼€å§‹å¤„ç†")
            
            # è·å–å½“å‰åº”ç”¨å®ä¾‹
            from flask import current_app
            app = current_app._get_current_object()
            
            # å¼‚æ­¥çº¿ç¨‹å¤„ç†é˜Ÿåˆ—
            def process_queue_thread():
                # åœ¨çº¿ç¨‹ä¸­åˆ›å»ºæ–°çš„åº”ç”¨ä¸Šä¸‹æ–‡
                with app.app_context():
                    try:
                        print("çº¿ç¨‹å·²å¯åŠ¨ï¼Œå¼€å§‹å¤„ç†é˜Ÿåˆ—...")
                        # å¤„ç†é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰å¾…å¤„ç†é¡¹ç›®
                        while True:
                            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¾…å¤„ç†é¡¹ç›®
                            pending_count = db.news_analysis_queue.count_documents({"status": "pending"})
                            processing_count = db.news_analysis_queue.count_documents({"status": "processing"})
                            
                            print(f"é˜Ÿåˆ—çŠ¶æ€: å¾…å¤„ç†={pending_count}, å¤„ç†ä¸­={processing_count}")
                            
                            # å¦‚æœæ²¡æœ‰å¾…å¤„ç†é¡¹ç›®å¹¶ä¸”æ²¡æœ‰å¤„ç†ä¸­çš„é¡¹ç›®ï¼Œç»“æŸå¾ªç¯
                            if pending_count == 0:
                                if processing_count > 0:
                                    print(f"ç­‰å¾… {processing_count} ä¸ªå¤„ç†ä¸­çš„ä»»åŠ¡å®Œæˆ...")
                                    time.sleep(5)  # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œç»™å¤„ç†ä¸­çš„ä»»åŠ¡å®Œæˆçš„æœºä¼š
                                    continue
                                else:
                                    print("é˜Ÿåˆ—ä¸­æ‰€æœ‰ä»»åŠ¡å·²å¤„ç†å®Œæˆ")
                                    break
                                
                            # å¦‚æœæœ‰å¤„ç†ä¸­çš„ä»»åŠ¡ï¼Œç­‰å¾…å®ƒä»¬å®Œæˆåå†å¤„ç†æ–°çš„æ‰¹æ¬¡
                            if processing_count > 0:
                                print(f"æœ‰ {processing_count} ä¸ªä»»åŠ¡æ­£åœ¨å¤„ç†ä¸­ï¼Œç­‰å¾…å®ƒä»¬å®Œæˆ...")
                                time.sleep(5)
                                continue
                                
                            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„å¤§å°
                            batch_size = min(10, pending_count)
                            print(f"å¼€å§‹å¤„ç†ä¸€æ‰¹{batch_size}æ¡æ–°é—»...")
                            
                            try:
                                # å¤„ç†æ–°çš„æ‰¹æ¬¡å¹¶ç­‰å¾…å…¶å®Œæˆ
                                result = NewsService.process_analysis_queue(max_workers=max_workers, limit=batch_size)
                                print(f"æ‰¹é‡å¤„ç†ç»“æœ: {result}")
                                
                                # ç­‰å¾…å¤„ç†å®Œæˆåï¼Œå†å¯åŠ¨ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
                                wait_count = 0
                                max_wait = 30  # æœ€å¤šç­‰å¾…30æ¬¡æ£€æŸ¥
                                
                                while db.news_analysis_queue.count_documents({"status": "processing"}) > 0:
                                    if wait_count >= max_wait:
                                        print("ç­‰å¾…å¤„ç†ä¸­ä»»åŠ¡å®Œæˆè¶…æ—¶ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹")
                                        break
                                        
                                    print("ç­‰å¾…å¤„ç†ä¸­çš„ä»»åŠ¡å®Œæˆ...")
                                    time.sleep(2)
                                    wait_count += 1
                                
                            except Exception as batch_error:
                                print(f"å¤„ç†æ‰¹æ¬¡æ—¶å‡ºé”™: {str(batch_error)}")
                                # ç»™ç³»ç»Ÿä¸€äº›æ¢å¤æ—¶é—´
                                time.sleep(5)
                            
                            # ç»™æœåŠ¡å™¨ä¸€äº›å–˜æ¯æ—¶é—´
                            time.sleep(2)
                    except Exception as e:
                        print(f"é˜Ÿåˆ—å¤„ç†çº¿ç¨‹å‡ºé”™: {str(e)}")
                        import traceback
                        traceback.print_exc()
            
            # å¯åŠ¨åå°çº¿ç¨‹å¤„ç†é˜Ÿåˆ—
            thread = threading.Thread(target=process_queue_thread)
            thread.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œä¸é˜»æ­¢ä¸»ç¨‹åºé€€å‡º
            thread.start()
            
            return {
                "status": "processing",
                "message": f"å¼€å§‹å¤„ç†é˜Ÿåˆ—ä¸­çš„ {pending_count} æ¡æ–°é—»",
                "thread_id": thread.ident
            }
            
        except Exception as e:
            print(f"ç«‹å³å¤„ç†é˜Ÿåˆ—å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}

    @staticmethod
    def process_analysis_queue(max_workers=16, limit=50):
        """
        å¤„ç†åˆ†æé˜Ÿåˆ—ä¸­çš„æ–°é—»ï¼Œå¼‚æ­¥æ‰§è¡Œåˆ†æä»»åŠ¡
        æ­¤æ–¹æ³•åº”è¯¥é€šè¿‡åå°ä»»åŠ¡è°ƒåº¦å™¨å®šæœŸè°ƒç”¨ï¼Œæˆ–é€šè¿‡ç®¡ç†å‘˜APIè§¦å‘
        
        Args:
            max_workers (int): æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            limit (int): å•æ¬¡å¤„ç†çš„æœ€å¤§æ–°é—»æ•°é‡
            
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        try:
            # ä½¿ç”¨åŸå­æ“ä½œæŸ¥æ‰¾å¾…å¤„ç†é¡¹ç›®å¹¶æ›´æ–°ä¸ºå¤„ç†ä¸­çŠ¶æ€
            # è¿™æ ·å¯ä»¥é¿å…å¤šä¸ªè¿›ç¨‹åŒæ—¶è¯»å–å’Œå¤„ç†åŒä¸€æ¡æ–°é—»
            current_time = datetime.now().isoformat()
            
            # æŸ¥æ‰¾å¹¶æ ‡è®°è¦å¤„ç†çš„é¡¹ç›®çš„åŸå­æ“ä½œ
            processing_ids = []
            pending_news = []
            
            # ä¿®æ”¹æŸ¥è¯¢æ–¹å¼ï¼Œä¼˜å…ˆå¤„ç†é«˜çƒ­åº¦æ–°é—»ï¼ˆæœ‰priority=highæ ‡è®°çš„ï¼‰
            # ç¬¬ä¸€è½®ï¼šæŸ¥æ‰¾é«˜ä¼˜å…ˆçº§çš„æ–°é—»
            for _ in range(limit):
                result = db.news_analysis_queue.find_one_and_update(
                    {"status": "pending", "priority": "high"},
                    {"$set": {
                        "status": "processing", 
                        "last_attempt": current_time
                    },
                    "$inc": {"attempts": 1}},
                    sort=[("queued_at", 1)],
                    return_document=True
                )
                
                if result:
                    pending_news.append(result)
                    processing_ids.append(result.get("news_id"))
                else:
                    # æ²¡æœ‰æ›´å¤šé«˜ä¼˜å…ˆçº§çš„å¾…å¤„ç†é¡¹ç›®
                    break
            
            # è®¡ç®—è¿˜éœ€è¦å¤„ç†å¤šå°‘æ™®é€šä¼˜å…ˆçº§çš„æ–°é—»
            remaining_limit = limit - len(pending_news)
            
            # ç¬¬äºŒè½®ï¼šå¦‚æœè¿˜æœ‰ç©ºé—´å¤„ç†æ™®é€šé¡¹ç›®ï¼ŒæŸ¥æ‰¾æ™®é€šä¼˜å…ˆçº§çš„æ–°é—»
            if remaining_limit > 0:
                for _ in range(remaining_limit):
                    result = db.news_analysis_queue.find_one_and_update(
                        {"status": "pending", "$or": [{"priority": {"$ne": "high"}}, {"priority": {"$exists": False}}]},
                        {"$set": {
                            "status": "processing", 
                            "last_attempt": current_time
                        },
                        "$inc": {"attempts": 1}},
                        sort=[("queued_at", 1)],
                        return_document=True
                    )
                    
                    if result:
                        pending_news.append(result)
                        processing_ids.append(result.get("news_id"))
                    else:
                        # æ²¡æœ‰æ›´å¤šå¾…å¤„ç†é¡¹ç›®
                        break
            
            if not pending_news:
                print("åˆ†æé˜Ÿåˆ—ä¸ºç©ºï¼Œæ— éœ€å¤„ç†")
                return {"status": "empty", "message": "åˆ†æé˜Ÿåˆ—ä¸ºç©º"}
            
            # åˆ†ç±»æ—¥å¿—è¾“å‡º
            high_priority_count = sum(1 for news in pending_news if news.get("priority") == "high")
            normal_priority_count = len(pending_news) - high_priority_count
            
            print(f"ä»é˜Ÿåˆ—ä¸­è·å–å¹¶é”å®š {len(pending_news)} æ¡å¾…åˆ†ææ–°é—» (é«˜ä¼˜å…ˆçº§: {high_priority_count}, æ™®é€šä¼˜å…ˆçº§: {normal_priority_count})")
            
            # è·å–å½“å‰åº”ç”¨å®ä¾‹ï¼Œç”¨äºè·å–é…ç½®
            from flask import current_app
            app = current_app._get_current_object()
            
            # 2. è·å–APIé…ç½®
            api_key = app.config.get('QWEN_API_KEY')
            base_url = app.config.get('QWEN_BASE_URL')
            model = app.config.get('QWEN_MODEL')
            
            if not api_key or not base_url or not model:
                print("APIé…ç½®ä¸å®Œæ•´ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                # é‡ç½®å¤„ç†ä¸­çŠ¶æ€
                for news_id in processing_ids:
                    db.news_analysis_queue.update_one(
                        {"news_id": news_id},
                        {"$set": {"status": "pending"}}
                    )
                return {"status": "error", "message": "APIé…ç½®ä¸å®Œæ•´"}
                
            print(f"APIé…ç½®: model={model}, base_url={base_url[:15]}...")
            
            # 3. åˆ›å»ºåˆ†ææœåŠ¡
            from .news_analysis_service import NewsAnalysisService
            try:
                analysis_service = NewsAnalysisService(api_key, base_url, model)
                print(f"æˆåŠŸåˆ›å»ºåˆ†ææœåŠ¡ï¼Œæ¨¡å‹: {model}")
                
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ¨¡æ‹Ÿå®¢æˆ·ç«¯
                if hasattr(analysis_service, 'use_mock') and analysis_service.use_mock:
                    print("æ³¨æ„ï¼šä½¿ç”¨çš„æ˜¯æ¨¡æ‹Ÿå®¢æˆ·ç«¯ï¼Œå°†ç”Ÿæˆæ¨¡æ‹Ÿåˆ†ææ•°æ®")
                    
                    # ä¸ºæ¯ä¸ªæ–°é—»ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
                    success_count = 0
                    for item in pending_news:
                        try:
                            news_data = item.get("news_data", {})
                            news_id = item.get("news_id")
                            title = news_data.get("title", "æœªçŸ¥æ ‡é¢˜")
                            platform = news_data.get("platform", "unknown")

                            
                            # ä½¿ç”¨analyze_newsæ–¹æ³•ï¼Œå®ƒä¼šå¤„ç†æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ
                            try:
                                result = analysis_service.analyze_news(news_data)
                                
                                # è®°å½•åˆ†ææ—¶é—´å’Œè§¦å‘æ–¹å¼
                                timestamp = datetime.now().isoformat()
                                result["analyzed_at"] = timestamp
                                result["analysis_trigger"] = item.get("priority", "normal")
                                
                                # ç”Ÿæˆå”¯ä¸€IDï¼ŒåŸºäºæ ‡é¢˜å’Œæ—¶é—´æˆ³
                                unique_id = f"{title}_{timestamp}"
                                result["id"] = hashlib.md5(unique_id.encode()).hexdigest()
                                
                                # å¯¹æ•°æ®æ’åºåä¿å­˜
                                if "comprehensive_heat" in result:
                                    result["rank"] = round(result["comprehensive_heat"] * 100)
                                
                                # ä¿å­˜åˆ°transformed_newsé›†åˆ
                                db.transformed_news.update_one(
                                    {"title": title},
                                    {"$set": result},
                                    upsert=True
                                )
                                
                                # æ›´æ–°åˆ†æè®°å½•
                                db.news_analysis_records.update_one(
                                    {"news_id": news_id},
                                    {"$set": {
                                        "news_id": news_id,
                                        "title": title,
                                        "analyzed_at": timestamp,
                                        "status": "completed",
                                        "priority": item.get("priority", "normal")
                                    }},
                                    upsert=True
                                )
                                
                                # ä»é˜Ÿåˆ—ä¸­ç§»é™¤
                                db.news_analysis_queue.delete_one({"news_id": news_id})
                                
                                success_count += 1
                                print(f"æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜æ¨¡æ‹Ÿåˆ†æ: {title[:30]}...")
                            except Exception as e:
                                print(f"ä¿å­˜æ¨¡æ‹Ÿåˆ†æå¤±è´¥: {str(e)}")
                                # å°†ä»»åŠ¡æ ‡è®°ä¸ºå¤±è´¥
                                db.news_analysis_queue.update_one(
                                    {"news_id": news_id},
                                    {"$set": {"status": "failed", "error": str(e)}}
                                )
                        except Exception as e:
                            print(f"å¤„ç†æ–°é—»æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                            continue
                    return {
                        "status": "success_mock",
                        "message": f"æˆåŠŸå¤„ç† {success_count}/{len(pending_news)} æ¡æ–°é—»ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰",
                        "processed": success_count,
                        "mock": True
                    }
            except Exception as e:
                print(f"åˆ›å»ºåˆ†ææœåŠ¡å¤±è´¥: {str(e)}")
                # å°†æ‰€æœ‰å¤„ç†ä¸­çš„æ–°é—»é‡ç½®ä¸ºå¾…å¤„ç†çŠ¶æ€
                for news_id in processing_ids:
                    db.news_analysis_queue.update_one(
                        {"news_id": news_id},
                        {"$set": {"status": "pending", "updated_at": datetime.now().isoformat()}}
                    )
                return {"status": "error", "message": f"åˆ›å»ºåˆ†ææœåŠ¡å¤±è´¥: {str(e)}"}
            
            # 4. å‡†å¤‡æ‰¹é‡åˆ†æ
            news_ids = []
            titles = []
            platforms = []
            priorities = []
            
            for item in pending_news:
                news_data = item.get("news_data", {})
                news_ids.append(item.get("news_id"))
                titles.append(news_data.get("title", ""))
                platforms.append(news_data.get("platform", "unknown"))
                priorities.append(item.get("priority", "normal"))
            
            print(f"å¼€å§‹åˆ†æ {len(titles)} æ¡æ–°é—»ï¼Œå…¶ä¸­é«˜ä¼˜å…ˆçº§: {priorities.count('high')}æ¡")
            
            # 5. ä½¿ç”¨ä¼˜åŒ–çš„å¤šçº¿ç¨‹åˆ†ææ–¹æ³•
            try:
                results = analysis_service.analyze_multiple_news(
                    titles, platforms, max_workers=max_workers
                )
                print(f"åˆ†æå®Œæˆï¼Œå¾—åˆ° {len(results)} æ¡ç»“æœ")
            except Exception as analysis_error:
                print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(analysis_error)}")
                # å°†çŠ¶æ€é‡ç½®ä¸ºå¾…å¤„ç†
                for news_id in news_ids:
                    db.news_analysis_queue.update_one(
                        {"news_id": news_id, "status": "processing"},
                        {"$set": {"status": "pending"}}
                    )
                return {"status": "error", "message": f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(analysis_error)}"}
            
            # 6. æ›´æ–°é˜Ÿåˆ—å’Œä¿å­˜ç»“æœ
            success_count = 0
            for i, result in enumerate(results):
                title = result.get("title", "")
                if not title:
                    continue
                    
                # è·å–å¯¹åº”çš„ç´¢å¼•
                idx = -1
                for j, t in enumerate(titles):
                    if t == title:
                        idx = j
                        break
                
                if idx == -1:
                    continue
                
                news_id = news_ids[idx]
                priority = priorities[idx]
                
                try:
                    # è®°å½•åˆ†ææ—¶é—´å’Œè§¦å‘æ–¹å¼
                    timestamp = datetime.now().isoformat()
                    result["analyzed_at"] = timestamp
                    result["analysis_trigger"] = priority
                    
                    # ç”Ÿæˆå”¯ä¸€IDï¼ŒåŸºäºæ ‡é¢˜å’Œæ—¶é—´æˆ³
                    unique_id = f"{title}_{timestamp}"
                    result["id"] = hashlib.md5(unique_id.encode()).hexdigest()
                    
                    # å¯¹æ•°æ®æ’åºåä¿å­˜
                    if "comprehensive_heat" in result:
                        result["rank"] = round(result["comprehensive_heat"] * 100)
                    
                    # ä¿å­˜åˆ°transformed_newsé›†åˆ
                    db.transformed_news.update_one(
                        {"title": title},
                        {"$set": result},
                        upsert=True
                    )
                    
                    # æ›´æ–°åˆ†æè®°å½•
                    db.news_analysis_records.update_one(
                        {"news_id": news_id},
                        {"$set": {
                            "news_id": news_id,
                            "title": title,
                            "analyzed_at": timestamp,
                            "status": "completed",
                            "priority": priority
                        }},
                        upsert=True
                    )
                    
                    # ä»é˜Ÿåˆ—ä¸­ç§»é™¤ - ç¡®ä¿åªåˆ é™¤çŠ¶æ€ä¸ºprocessingçš„é¡¹ç›®
                    db.news_analysis_queue.delete_one({
                        "news_id": news_id,
                        "status": "processing"
                    })
                    
                    success_count += 1
                    print(f"æˆåŠŸä¿å­˜æ–°é—»åˆ†æ: {title[:30]}... (ä¼˜å…ˆçº§: {priority})")
                except Exception as save_error:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤é”®é”™è¯¯
                    if "duplicate key error" in str(save_error):
                        print(f"ä¿å­˜æ–°é—»åˆ†æç»“æœå¤±è´¥ {title[:30]}: {str(save_error)}")
                        try:
                            # ç”Ÿæˆæ–°çš„ID
                            result["id"] = hashlib.md5((title + str(time.time())).encode()).hexdigest()
                            # é‡è¯•
                            db.transformed_news.update_one(
                                {"title": title},
                                {"$set": result},
                                upsert=True
                            )
                            # ä»é˜Ÿåˆ—ä¸­ç§»é™¤ - ç¡®ä¿åªåˆ é™¤çŠ¶æ€ä¸ºprocessingçš„é¡¹ç›®
                            db.news_analysis_queue.delete_one({
                                "news_id": news_id,
                                "status": "processing"
                            })
                            success_count += 1
                            print(f"æˆåŠŸä½¿ç”¨æ–°IDä¿å­˜æ–°é—»åˆ†æ: {title[:30]}...")
                        except Exception as retry_error:
                            print(f"ä½¿ç”¨æ–°IDä¿å­˜å¤±è´¥: {str(retry_error)}")
                            # å°†é¡¹ç›®æ ‡è®°ä¸ºå¤±è´¥
                            db.news_analysis_queue.update_one(
                                {"news_id": news_id, "status": "processing"},
                                {"$set": {"status": "failed", "error": str(retry_error)}}
                            )
                    else:
                        print(f"ä¿å­˜æ–°é—»åˆ†æç»“æœå¤±è´¥ {title[:30]}: {str(save_error)}")
                        # å°†é¡¹ç›®æ ‡è®°ä¸ºå¤±è´¥
                        db.news_analysis_queue.update_one(
                            {"news_id": news_id, "status": "processing"},
                            {"$set": {"status": "failed", "error": str(save_error)}}
                        )
            
            # ç»Ÿè®¡é«˜ä¼˜å…ˆçº§å’Œæ™®é€šä¼˜å…ˆçº§çš„æˆåŠŸæ•°é‡
            high_success = 0
            normal_success = 0
            for i in range(success_count):
                if i < len(priorities) and priorities[i] == "high":
                    high_success += 1
                else:
                    normal_success += 1
                    
            print(f"æˆåŠŸåˆ†æå¹¶ä¿å­˜ {success_count}/{len(pending_news)} æ¡æ–°é—» (é«˜ä¼˜å…ˆçº§: {high_success}, æ™®é€šä¼˜å…ˆçº§: {normal_success})")
            
            # 7. å¤„ç†å¤±è´¥çš„æ–°é—»
            failed_count = 0
            for i, news_id in enumerate(news_ids):
                # æ£€æŸ¥æ˜¯å¦ä»åœ¨é˜Ÿåˆ—ä¸­å¹¶ä¸”çŠ¶æ€ä¸ºprocessing
                queue_item = db.news_analysis_queue.find_one({
                    "news_id": news_id,
                    "status": "processing"
                })
                
                if queue_item:
                    attempts = queue_item.get("attempts", 0)
                    priority = queue_item.get("priority", "normal")
                    
                    if attempts >= 3:  # æœ€å¤šå°è¯•3æ¬¡
                        # è¶…è¿‡æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œæ ‡è®°ä¸ºå¤±è´¥
                        db.news_analysis_queue.update_one(
                            {"news_id": news_id, "status": "processing"},
                            {"$set": {
                                "status": "failed",
                                "updated_at": datetime.now().isoformat(),
                                "error": "è¶…è¿‡æœ€å¤§å°è¯•æ¬¡æ•°"
                            }}
                        )
                        failed_count += 1
                    else:
                        # é‡æ–°æ ‡è®°ä¸ºå¾…å¤„ç†ï¼Œä½†ä¿æŒä¼˜å…ˆçº§
                        db.news_analysis_queue.update_one(
                            {"news_id": news_id, "status": "processing"},
                            {"$set": {
                                "status": "pending",
                                "priority": priority,  # ä¿æŒåŸæœ‰ä¼˜å…ˆçº§
                                "updated_at": datetime.now().isoformat()
                            }}
                        )
            
            return {
                "status": "success",
                "message": f"æˆåŠŸå¤„ç†åˆ†æé˜Ÿåˆ—ä¸­çš„ {success_count}/{len(pending_news)} æ¡æ–°é—»",
                "processed": success_count,
                "high_priority_processed": high_success,
                "normal_priority_processed": normal_success,
                "failed": failed_count
            }
            
        except Exception as e:
            print(f"å¤„ç†åˆ†æé˜Ÿåˆ—å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            
            # å°†æ‰€æœ‰æ­£åœ¨å¤„ç†çš„é¡¹ç›®é‡ç½®ä¸ºå¾…å¤„ç†
            try:
                db.news_analysis_queue.update_many(
                    {"status": "processing"},
                    {"$set": {"status": "pending", "updated_at": datetime.now().isoformat()}}
                )
                print("å·²å°†æ‰€æœ‰å¤„ç†ä¸­çš„æ–°é—»é‡ç½®ä¸ºå¾…å¤„ç†çŠ¶æ€")
            except Exception as reset_error:
                print(f"é‡ç½®å¤„ç†çŠ¶æ€å¤±è´¥: {str(reset_error)}")
                
            return {"status": "error", "message": str(e)}

    @staticmethod
    def cleanup_old_queue_items(max_age_hours=48):
        """
        æ¸…ç†åˆ†æé˜Ÿåˆ—ä¸­çš„æ—§é¡¹ç›®
        
        Args:
            max_age_hours (int): æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            int: åˆ é™¤çš„é¡¹ç›®æ•°é‡
        """
        try:
            cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
            
            # åˆ é™¤å·²å®Œæˆæˆ–å¤±è´¥çš„æ—§é¡¹ç›®
            result = db.news_analysis_queue.delete_many({
                "$or": [
                    {"status": "failed", "queued_at": {"$lt": cutoff_time.isoformat()}},
                    {"status": "completed", "queued_at": {"$lt": cutoff_time.isoformat()}}
                ]
            })
            
            deleted_count = result.deleted_count
            print(f"å·²æ¸…ç† {deleted_count} æ¡æ—§çš„åˆ†æé˜Ÿåˆ—é¡¹ç›®")
            return deleted_count
            
        except Exception as e:
            print(f"æ¸…ç†åˆ†æé˜Ÿåˆ—å¤±è´¥: {str(e)}")
            return 0

    @staticmethod
    def ensure_scheduled_tasks_running(app=None):
        """
        ç¡®ä¿åå°ä»»åŠ¡å·²å¯åŠ¨
        
        Args:
            app (Flask): Flaskåº”ç”¨å®ä¾‹
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå¯åŠ¨
        """
        try:
            from flask import current_app
            app_instance = app or current_app
            
            if not app_instance.config.get('SCHEDULED_TASKS_STARTED', False):
                from ..scheduled_tasks import start_scheduled_tasks
                with app_instance.app_context():
                    start_scheduled_tasks(app_instance)
                app_instance.config['SCHEDULED_TASKS_STARTED'] = True
                print("å·²å¯åŠ¨åå°æ–°é—»é‡‡é›†å’Œåˆ†æä»»åŠ¡")
            return True
        except Exception as e:
            print(f"å¯åŠ¨åå°ä»»åŠ¡å¤±è´¥: {str(e)}")
            return False

    @staticmethod
    def analyze_multiple_news(titles, platforms, max_workers=16):
        """
        åˆ†æå¤šä¸ªæ–°é—»æ ‡é¢˜ï¼Œå¤„ç†çƒ­é—¨æ–°é—»
        
        Args:
            titles (list): æ–°é—»æ ‡é¢˜åˆ—è¡¨
            platforms (list): å¹³å°åˆ—è¡¨ï¼Œé•¿åº¦åº”ä¸titlesä¸€è‡´
            max_workers (int): æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            list: åˆ†æç»“æœ
        """
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å¤„ç†å¥½çš„çƒ­é—¨æ–°é—»æ•°æ®
        latest_processed = NewsCollectionService.get_latest_processed_news()
        if latest_processed:
            # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿæ–°ï¼ˆ1å°æ—¶å†…ï¼‰
            timestamp = latest_processed.get("timestamp")
            if isinstance(timestamp, str):
                try:
                    timestamp_datetime = datetime.fromisoformat(timestamp)
                    time_diff = datetime.now() - timestamp_datetime
                    
                    # å¦‚æœæ•°æ®å°‘äº1å°æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨å¤„ç†å¥½çš„æ•°æ®
                    if time_diff.total_seconds() < 3600:
                        print("ä½¿ç”¨1å°æ—¶å†…å·²å¤„ç†å¥½çš„çƒ­é—¨æ–°é—»æ•°æ®")
                        
                        # ä»å¤„ç†å¥½çš„æ•°æ®ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ ‡é¢˜
                        processed_news = []
                        title_set = set(titles)
                        
                        for platform in latest_processed.get("data", []):
                            for news in platform.get("data", []):
                                if news.get("title") in title_set:
                                    processed_news.append(news)
                        
                        # å¦‚æœæ‰¾åˆ°äº†æ‰€æœ‰æ ‡é¢˜ï¼Œç›´æ¥è¿”å›
                        if len(processed_news) >= len(titles):
                            print(f"ä»å¤„ç†å¥½çš„æ•°æ®ä¸­æ‰¾åˆ°äº†æ‰€æœ‰{len(titles)}ä¸ªæ ‡é¢˜")
                            return processed_news
                        
                        # å¦‚æœæ‰¾åˆ°äº†éƒ¨åˆ†æ ‡é¢˜ï¼Œåªåˆ†æå‰©ä½™çš„æ ‡é¢˜
                        found_titles = set(news.get("title") for news in processed_news)
                        remaining_titles = []
                        remaining_platforms = []
                        
                        for i, title in enumerate(titles):
                            if title not in found_titles:
                                remaining_titles.append(title)
                                if i < len(platforms):
                                    remaining_platforms.append(platforms[i])
                                else:
                                    remaining_platforms.append("unknown")
                        
                        print(f"ä»å¤„ç†å¥½çš„æ•°æ®ä¸­æ‰¾åˆ°äº†{len(processed_news)}ä¸ªæ ‡é¢˜ï¼Œéœ€è¦åˆ†æå‰©ä½™çš„{len(remaining_titles)}ä¸ªæ ‡é¢˜")
                        
                        # åˆ†æå‰©ä½™çš„æ ‡é¢˜
                        if remaining_titles:
                            new_analyzed = NewsService._analyze_news_titles(
                                remaining_titles, remaining_platforms, max_workers
                            )
                            return processed_news + new_analyzed
                        
                        return processed_news
                except (ValueError, TypeError):
                    pass
        
        # å¦‚æœæ²¡æœ‰æœ€æ–°å¤„ç†å¥½çš„æ•°æ®ï¼Œä½¿ç”¨åŸå§‹åˆ†ææ–¹æ³•
        return NewsService._analyze_news_titles(titles, platforms, max_workers)
    
    @staticmethod
    def _analyze_news_titles(titles, platforms, max_workers=16):
        """
        åˆ†ææ–°é—»æ ‡é¢˜çš„åŸå§‹æ–¹æ³•
        """
        try:
            import random
            from datetime import datetime, timedelta
            import hashlib
            
            print(f"ä½¿ç”¨æµ‹è¯•æ–¹æ³•å¤„ç†{len(titles)}æ¡æ–°é—»")
            results = []
            
            for i, title in enumerate(titles):
                if not title:
                    continue
                
                platform = platforms[i] if i < len(platforms) else "unknown"
                news_id = hashlib.md5(title.encode()).hexdigest()
                
                # ç”Ÿæˆéšæœºæƒ…æ„Ÿæ•°æ®
                emotion_schema = {
                    "å–œæ‚¦": round(random.uniform(0, 30), 1),
                    "æœŸå¾…": round(random.uniform(0, 30), 1),
                    "å¹³å’Œ": round(random.uniform(0, 30), 1),
                    "æƒŠè®¶": round(random.uniform(0, 30), 1),
                    "æ‚²ä¼¤": round(random.uniform(0, 30), 1),
                    "æ„¤æ€’": round(random.uniform(0, 30), 1),
                    "ææƒ§": round(random.uniform(0, 30), 1),
                    "åŒæ¶": round(random.uniform(0, 30), 1)
                }
                
                # ç”Ÿæˆéšæœºç«‹åœºæ•°æ®
                stance_schema = {
                    "ç§¯æå€¡å¯¼": round(random.uniform(5, 30), 1),
                    "å¼ºçƒˆåå¯¹": round(random.uniform(5, 20), 1),
                    "ä¸­ç«‹é™ˆè¿°": round(random.uniform(10, 30), 1),
                    "è´¨ç–‘æ¢ç©¶": round(random.uniform(5, 20), 1),
                    "ç†æ€§å»ºè®®": round(random.uniform(5, 20), 1),
                    "æƒ…ç»ªå®£æ³„": round(random.uniform(5, 15), 1),
                    "è§‚æœ›ç­‰å¾…": round(random.uniform(5, 15), 1),
                    "æ‰©æ•£ä¼ æ’­": round(random.uniform(5, 15), 1)
                }
                
                # ç”Ÿæˆéšæœºçƒ­åº¦è¶‹åŠ¿
                now = datetime.now()
                heat_trend = []
                for j in range(5):
                    date = (now - timedelta(days=j)).strftime("%Y-%m-%d")
                    heat_trend.append({
                        "date": date,
                        "value": round(random.uniform(0.5, 1.0), 2)
                    })
                
                # ç”Ÿæˆéšæœºè¯äº‘
                word_cloud = []
                common_words = ["æ–°é—»", "çƒ­ç‚¹", "äº‹ä»¶", "å…³æ³¨", "ç¤¾ä¼š", "å½±å“", "å‘å±•", 
                               "å˜åŒ–", "è¶‹åŠ¿", "åˆ†æ", "æŠ¥é“", "åª’ä½“", "ä¼ æ’­", "èˆ†è®º"]
                
                for word in common_words:
                    word_cloud.append({
                        "word": word,
                        "weight": round(random.uniform(0.1, 1.0), 2)
                    })
                
                # ç»„è£…ç»“æœ
                result = {
                    "id": news_id,
                    "title": title,
                    "summary": f"{title}çš„å†…å®¹æ‘˜è¦...",
                    "category": random.choice(["æ—¶æ”¿", "ç§‘æŠ€", "è´¢ç»", "ç¤¾ä¼š", "å›½é™…"]),
                    "source": platform,
                    "location": {
                        "x": round(random.uniform(73, 135), 6),
                        "y": round(random.uniform(18, 53), 6)
                    },
                    "analyzed_at": datetime.now().isoformat(),
                    "spread_metrics": {
                        "speed": round(random.uniform(0.5, 1.0), 2),
                        "range": round(random.uniform(0.5, 1.0), 2),
                        "participants": round(random.uniform(0.5, 1.0), 2)
                    },
                    "emotion_analysis": {
                        "schema": emotion_schema,
                        "rationale": f"{title}å¼•å‘çš„æƒ…æ„Ÿä¸»è¦æ˜¯æœŸå¾…å’Œå¹³å’Œ"
                    },
                    "stance_analysis": {
                        "schema": stance_schema,
                        "rationale": f"å¯¹{title}çš„ç«‹åœºä¸»è¦æ˜¯ç§¯æå€¡å¯¼å’Œä¸­ç«‹é™ˆè¿°"
                    },
                    "heat_trend": heat_trend,
                    "timeline": [
                        {
                            "date": (now - timedelta(days=2)).strftime("%Y-%m-%d"),
                            "event": f"{title}é¦–æ¬¡è¢«æŠ¥é“"
                        },
                        {
                            "date": now.strftime("%Y-%m-%d"),
                            "event": f"{title}å¼•å‘çƒ­è®®"
                        }
                    ],
                    "wordCloud": word_cloud,
                    "heat": round(random.uniform(60, 100), 2),
                    "is_fallback": True  # æ ‡è®°ä¸ºåå¤‡æ•°æ®
                }
                
                results.append(result)
                print(f"ç”Ÿæˆæµ‹è¯•æ•°æ®: {title[:30]}...")
                
                # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´ï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(0.1)
            
            print(f"æˆåŠŸç”Ÿæˆ{len(results)}æ¡æµ‹è¯•æ•°æ®")
            return results
            
        except Exception as e:
            print(f"ç”Ÿæˆæµ‹è¯•æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            return []

    @classmethod
    def update_current_hot_news(cls, n=None):
        """
        ä»hot_news_processedä¸­æ‰¾å‡ºæœ€æ–°çš„è®°å½•ï¼Œä»data[0].dataé‡Œæ‰¾å‡ºcomprehensive_heatæœ€é«˜çš„å‰næ¡çƒ­æœï¼Œ
        ç„¶åæ ¹æ®æ ‡é¢˜åœ¨transformed_newsä¸­æŸ¥æ‰¾æœ€æ–°çš„åˆ†æç»“æœï¼Œ
        å°†è¿™äº›ç»“æœåŸå°ä¸åŠ¨åœ°è¦†ç›–current_hot_newsè¡¨å†…å®¹
        
        æ³¨æ„: é™¤äº†å®šæ—¶ä»»åŠ¡å¤–ï¼Œè¯¥åŠŸèƒ½ä¹Ÿå¯ä»¥åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶é€šè¿‡å‘½ä»¤è¡Œå‚æ•° --update-cache æ‰‹åŠ¨è§¦å‘
        """
        try:
            from flask import current_app
            from app.extensions import db
            
            # ä»é…ç½®ä¸­è·å–é»˜è®¤å€¼
            if n is None:
                n = current_app.config.get('TOP_HOT_NEWS_COUNT', 20)
                
            print(f"[{datetime.now()}] å¼€å§‹æ›´æ–°å‰{n}æ¡çƒ­æœæ–°é—»ç¼“å­˜...")
            
            # æ­¥éª¤1: ä»hot_news_processedè·å–æœ€æ–°çš„è®°å½•
            latest_record = db.hot_news_processed.find_one(
                sort=[("timestamp", pymongo.DESCENDING)]
            )
            
            if not latest_record:
                print("æœªæ‰¾åˆ°çƒ­æœæ–°é—»è®°å½•")
                return {"status": "error", "message": "No hot news records found", "count": 0}
                
            # æ­¥éª¤2: ä»æœ€æ–°è®°å½•çš„data[0].dataä¸­è·å–å‰næ¡çƒ­æœï¼ˆæŒ‰comprehensive_heatæ’åºï¼‰
            try:
                hot_news_data = latest_record.get('data', [{}])[0].get('data', [])
                if not hot_news_data:
                    print("çƒ­æœæ•°æ®ä¸ºç©º")
                    return {"status": "error", "message": "Hot news data is empty", "count": 0}
                    
                # æŒ‰comprehensive_heatæ’åºå¹¶è·å–å‰næ¡
                sorted_news = sorted(hot_news_data, key=lambda x: x.get('comprehensive_heat', 0), reverse=True)
                top_news = sorted_news[:n]
                
                print(f"æ‰¾åˆ°{len(top_news)}æ¡çƒ­æœæ–°é—»ï¼ˆä»æ€»è®¡{len(hot_news_data)}æ¡ä¸­ï¼‰")
                
                # æå–æ ‡é¢˜
                top_news_titles = [news.get('title') for news in top_news if news.get('title')]
                print(f"æå–äº†{len(top_news_titles)}ä¸ªæœ‰æ•ˆæ ‡é¢˜")
                
                # åˆ›å»ºæ ‡é¢˜åˆ°å½“å‰çƒ­åº¦æ•°æ®çš„æ˜ å°„
                title_to_current_heat = {}
                for news in top_news:
                    if news.get('title'):
                        title_to_current_heat[news.get('title')] = {
                            'comprehensive_heat': news.get('comprehensive_heat', 0),
                            'timestamp': latest_record.get('timestamp')
                        }
                
                # è·å–å†å²çƒ­åº¦æ•°æ®ï¼ˆè¿‡å»7å¤©ï¼‰
                print("å¼€å§‹æ”¶é›†å†å²çƒ­åº¦æ•°æ®...")
                title_to_heat_history = {}
                
                # è®¡ç®—7å¤©å‰çš„æ—¶é—´
                seven_days_ago = datetime.now() - timedelta(days=7)
                
                # è·å–è¿‡å»7å¤©çš„çƒ­åº¦è®°å½•
                historical_records = list(db.hot_news_processed.find(
                    {"timestamp": {"$gte": seven_days_ago.isoformat()}},
                    sort=[("timestamp", pymongo.ASCENDING)]
                ))
                
                print(f"æ‰¾åˆ°{len(historical_records)}æ¡å†å²çƒ­åº¦è®°å½•")
                
                # ä¸ºæ¯ä¸ªçƒ­é—¨æ ‡é¢˜æ”¶é›†å†å²çƒ­åº¦æ•°æ®
                for title in top_news_titles:
                    heat_history = []
                    
                    # éå†å†å²è®°å½•
                    for record in historical_records:
                        if not record.get('data') or not record.get('data')[0].get('data'):
                            continue
                            
                        record_timestamp = record.get('timestamp')
                        record_data = record.get('data', [{}])[0].get('data', [])
                        
                        # åœ¨è®°å½•ä¸­æŸ¥æ‰¾åŒ¹é…æ ‡é¢˜çš„æ–°é—»
                        for news_item in record_data:
                            if news_item.get('title') == title:
                                heat_history.append({
                                    'timestamp': record_timestamp,
                                    'comprehensive_heat': news_item.get('comprehensive_heat', 0),
                                    'weighted_heat_value': news_item.get('weighted_heat_value', 0),
                                    'normalized_heat': news_item.get('normalized_heat', 0)
                                })
                                break
                    
                    title_to_heat_history[title] = heat_history
                    print(f"ä¸ºæ–°é—»ã€Š{title}ã€‹æ”¶é›†äº†{len(heat_history)}æ¡å†å²çƒ­åº¦æ•°æ®")
                
            except (IndexError, KeyError, TypeError) as e:
                print(f"è§£æçƒ­æœæ•°æ®æ—¶å‡ºé”™: {str(e)}")
                traceback.print_exc()
                return {"status": "error", "message": f"Error parsing hot news data: {str(e)}", "count": 0}
            
            if not top_news_titles:
                print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„çƒ­æœæ–°é—»æ ‡é¢˜")
                return {"status": "error", "message": "No valid hot news titles found", "count": 0}
            
            # æ­¥éª¤3: æ ¹æ®æ ‡é¢˜è·å–æœ€æ–°çš„å®Œæ•´åˆ†æç»“æœ
            latest_analyses = []
            i = 1
            for title in top_news_titles:
                # æŸ¥æ‰¾è¯¥æ ‡é¢˜æœ€æ–°çš„åˆ†æç»“æœ
                analysis = db.transformed_news.find_one(
                    {"title": title},
                    sort=[("analyzed_at", -1)]  # æŒ‰åˆ†ææ—¶é—´é™åº
                )
                
                if analysis:
                    # ç§»é™¤MongoDBçš„_idå­—æ®µï¼Œé¿å…æ’å…¥é”™è¯¯
                    if "_id" in analysis:
                        del analysis["_id"]
                    analysis["rank"] = i
                    i += 1
                    
                    # æ·»åŠ çƒ­åº¦å†å²æ•°æ®
                    if title in title_to_heat_history:
                        analysis["heat_history"] = title_to_heat_history[title]
                        print(f"ä¸ºæ–°é—»ã€Š{title}ã€‹æ·»åŠ äº†çƒ­åº¦å†å²æ•°æ®")
                    else:
                        analysis["heat_history"] = []
                        print(f"æœªæ‰¾åˆ°æ–°é—»ã€Š{title}ã€‹çš„çƒ­åº¦å†å²æ•°æ®")
                    
                    latest_analyses.append(analysis)
                else:
                    print(f"æœªæ‰¾åˆ°æ–°é—»ã€Š{title}ã€‹çš„åˆ†æç»“æœ")
            
            if not latest_analyses:
                print("æœªæ‰¾åˆ°ä»»ä½•åˆ†æç»“æœ")
                return {"status": "error", "message": "No analysis results found", "count": 0}
            
            # æ­¥éª¤4: ä¿å­˜ç°æœ‰æ•°æ®åˆ°å†å²è®°å½•å¹¶æ¯”è¾ƒæ’åå˜åŒ–
            try:
                # è·å–å½“å‰çš„çƒ­æœæ–°é—»æ•°æ®ï¼ˆå³å°†è¢«åˆ é™¤çš„æ•°æ®ï¼‰
                current_hot_news = list(db.current_hot_news.find())
                print(f"è·å–åˆ°{len(current_hot_news)}æ¡ç°æœ‰çƒ­æœæ•°æ®")
                
                # å¦‚æœæœ‰ç°æœ‰æ•°æ®ï¼Œå°†å…¶ä¿å­˜åˆ°å†å²è®°å½•ä¸­
                if current_hot_news:
                    # ç»™å†å²æ•°æ®æ·»åŠ æ—¶é—´æˆ³
                    timestamp = datetime.now().isoformat()
                    for item in current_hot_news:
                        if "_id" in item:
                            item["original_id"] = str(item["_id"])
                            del item["_id"]
                        item["archived_at"] = timestamp
                    
                    # æ’å…¥åˆ°å†å²è®°å½•é›†åˆ
                    history_result = db.history_top_news.insert_many(current_hot_news)
                    history_count = len(history_result.inserted_ids) if hasattr(history_result, 'inserted_ids') else 0
                    print(f"æˆåŠŸä¿å­˜{history_count}æ¡å†å²çƒ­æœæ•°æ®")
                    
                    # åˆ›å»ºæ ‡é¢˜åˆ°æ’åçš„æ˜ å°„ï¼Œç”¨äºæ¯”è¾ƒæ’åå˜åŒ–
                    title_to_old_rank = {item.get("title"): item.get("rank") for item in current_hot_news if item.get("title") and item.get("rank")}
                    
                    # ä¸ºæ–°æ•°æ®æ·»åŠ æ’åå˜åŒ–ä¿¡æ¯
                    for analysis in latest_analyses:
                        title = analysis.get("title")
                        new_rank = analysis.get("rank")
                        if title in title_to_old_rank and new_rank is not None:
                            old_rank = title_to_old_rank[title]
                            if new_rank < old_rank:
                                analysis["rank_change"] = "up"  # æ’åä¸Šå‡ï¼ˆæ•°å­—å˜å°ï¼‰
                                print(f"æ–°é—»ã€Š{title}ã€‹æ’åä¸Šå‡: {old_rank} -> {new_rank}")
                            elif new_rank > old_rank:
                                analysis["rank_change"] = "down"  # æ’åä¸‹é™ï¼ˆæ•°å­—å˜å¤§ï¼‰
                                print(f"æ–°é—»ã€Š{title}ã€‹æ’åä¸‹é™: {old_rank} -> {new_rank}")
                            else:
                                analysis["rank_change"] = "same"  # æ’åä¸å˜
                                print(f"æ–°é—»ã€Š{title}ã€‹æ’åä¸å˜: ä¿æŒåœ¨ç¬¬{new_rank}å")
                        else:
                            analysis["rank_change"] = "new"  # æ–°ä¸Šæ¦œ
                            print(f"æ–°é—»ã€Š{title}ã€‹æ–°ä¸Šæ¦œ: ç¬¬{new_rank}å")
                else:
                    print("å½“å‰æ²¡æœ‰çƒ­æœæ•°æ®ï¼Œæ— éœ€ä¿å­˜å†å²è®°å½•")
                    # æ‰€æœ‰æ–°æ•°æ®æ ‡è®°ä¸ºæ–°ä¸Šæ¦œ
                    for analysis in latest_analyses:
                        analysis["rank_change"] = "new"
            
            except Exception as e:
                print(f"å¤„ç†å†å²æ•°æ®å’Œæ’åå˜åŒ–æ—¶å‡ºé”™: {str(e)}")
                traceback.print_exc()
                # ç»§ç»­æ‰§è¡Œï¼Œä¸å½±å“ä¸»æµç¨‹
            
            # æ­¥éª¤5: æ¸…ç©ºcurrent_hot_newsè¡¨å¹¶æ’å…¥æ–°æ•°æ®
            try:
                # åˆ é™¤æ—§æ•°æ®
                delete_result = db.current_hot_news.delete_many({})
                print(f"åˆ é™¤äº†{delete_result.deleted_count}æ¡æ—§æ•°æ®")
                
                # æ’å…¥æ–°æ•°æ®
                insert_result = db.current_hot_news.insert_many(latest_analyses)
                inserted_count = len(insert_result.inserted_ids) if hasattr(insert_result, 'inserted_ids') else 0
                print(f"æ’å…¥äº†{inserted_count}æ¡æ–°æ•°æ®")
            except Exception as e:
                print(f"æ›´æ–°æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
                traceback.print_exc()
                return {"status": "error", "message": str(e), "count": 0}
            
            print(f"[{datetime.now()}] æˆåŠŸæ›´æ–°{len(latest_analyses)}æ¡çƒ­æœæ–°é—»åˆ†æç»“æœåˆ°ç¼“å­˜è¡¨")
            
            return {
                "status": "success", 
                "timestamp": latest_record.get('timestamp'),
                "total_hot_news": len(top_news),
                "matched_analysis_count": len(latest_analyses),
                "updated_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"æ›´æ–°çƒ­æœæ–°é—»ç¼“å­˜å¤±è´¥: {str(e)}")
            traceback.print_exc()
            return {"status": "error", "message": str(e), "count": 0} 